{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#DL4H_Team_38 Final Project - Draft"
      ],
      "metadata": {
        "id": "XJUP3QGfncWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Authors**:\n",
        "####Eric Chen(echen40), Li-Kai Lin(likaikl2), Peng-Yuan Huang(pyh2)\n",
        "\n",
        "---\n",
        "#####Github file link: https://github.com/likaikl2/DLH_Team38_draft/blob/main/DL4H_Team_38.ipynb"
      ],
      "metadata": {
        "id": "NXlfNDKuue8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install gdown\n",
        "\n",
        "!gdown https://drive.google.com/drive/folders/1Ab5KF4r31fQjtotlTgmlvwRqt1RenXCo?usp=sharing -O /content/drive/MyDrive/Colab_Notebooks/Data --folder\n",
        "\n"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "The goal of this study is to predict heart failure risk in patients based on their electronic health record (EHR) data. Early identification of high-risk patients could enable timely interventions and improved outcomes. However, accurately predicting heart failure risk remains challenging,\n",
        "especially when EHR data is limited. This study aims to validate whether the Domain Knowledge Guided Recurrent Neural Networks (DG-RNN) model, which incorporates medical domain knowledge via a knowledge graph and utilizes various architectural components, can outperform existing risk prediction models and provide interpretable results.\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
        "\n",
        "1. Hypothesis 1: Incorporating domain knowledge from a medical knowledge graph (KnowLife) into a deep\n",
        "learning model (DG-RNN) will significantly improve the performance of clinical risk\n",
        "prediction compared to models that do not utilize domain knowledge.\n",
        "2. Hypothesis 2: The dynamic attention mechanism in DG-RNN, which integrates relevant information from\n",
        "the medical knowledge graph at each step, will contribute to better risk prediction\n",
        "performance compared to models with static or no attention mechanisms.\n",
        "3. Hypothesis 3: The global max pooling operation in DG-RNN will not only improve the model's\n",
        "performance but also enable the interpretation of individual medical event contributions to\n",
        "the predicted risk.\n",
        "4. Hypothesis 4: DG-RNN will demonstrate robustness to limited training data, outperforming baseline\n",
        "models even when the amount of available training data is reduced.\n",
        "5. Hypothesis 5: The incorporation of time encodings in DG-RNN, which capture the irregular time intervals\n",
        "between EHR events, will lead to improved risk prediction performance compared to\n",
        "models that do not consider temporal information.\n",
        "6. Hypothesis 6: The pre-training of medical concept and relation embeddings using TransE will provide a\n",
        "meaningful initialization for DG-RNN, contributing to its superior performance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The DG-RNN model predicts clinical risk by integrating medical events, time information, and domain knowledge from a medical knowledge graph. The model's inputs include medical event embeddings and corresponding time encoding vectors. Time encodings, similar to positional encodings in Transformer, capture the relative time to the prediction date and intervals between events. The medical event embeddings and time encodings are fed into an LSTM layer, which generates output vectors and hidden states at each step.\n",
        "\n",
        "A knowledge graph attention module then takes the hidden state and a sub-graph of the medical knowledge graph adjacent to the current event as inputs, producing an attention vector that is fed back into the LSTM. This allows the model to dynamically incorporate relevant domain knowledge for each event. The knowledge graph contains medical concepts and relationships, with embeddings initialized using TransE.\n",
        "\n",
        "The LSTM output vectors are concatenated and passed through a global max pooling layer, which improves performance and allows for interpretation of individual event contributions. Finally, a fully connected layer produces the clinical risk prediction.\n",
        "\n",
        "The model's interpretability lies in its ability to compute the contribution of each input event to the final risk prediction by analyzing the max-pooling layer's outputs and the corresponding fully connected layer's outputs. This is achieved through the global max pooling operation, which associates each feature of the final representation with a specific input event.\n",
        "\n",
        "The training process involves initializing embeddings, iterating through patient EHR data, computing time encodings, performing LSTM and attention operations, and updating parameters based on the prediction loss. The model's objective function is the cross-entropy between the ground truth and the predicted risk probability.\n"
      ],
      "metadata": {
        "id": "t6ScHIUlzIUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n"
      ],
      "metadata": {
        "id": "zEnsov5k7aQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python version"
      ],
      "metadata": {
        "id": "q56tTEBV8GiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(\"Python version\")\n",
        "print(sys.version)"
      ],
      "metadata": {
        "id": "5RBHSJs-8Qx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Dependencies/packages needed"
      ],
      "metadata": {
        "id": "Sy-tv7oz8fH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "# below imported from the initial section of main.py\n",
        "\n",
        "# sys.setdefaultencoding('utf8')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import random\n",
        "import json\n",
        "import traceback # imported from py_op.py\n",
        "import collections # imported from data_loader.py\n",
        "from collections import OrderedDict\n",
        "from tqdm import tqdm\n",
        "import warnings # imported from data_loader.py\n",
        "from copy import deepcopy # imported from data_loader.py\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "from torch.autograd import Variable\n",
        "from torch.backends import cudnn\n",
        "from torch.nn import DataParallel\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset # imported from data_loader.py\n",
        "import argparse #imported from parse.py\n",
        "\n"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The py_op module, which is from the file py_op.py, is imported as below and includes additional operations and utilities that support the model's training and evaluation:"
      ],
      "metadata": {
        "id": "t9AvxnDWQx20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# below imported from py_op.py\n",
        "def mywritejson(save_path,content):\n",
        "    content = json.dumps(content,indent=4,ensure_ascii=False)\n",
        "    with open(save_path,'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "def myreadjson(load_path):\n",
        "    with open(load_path,'r') as f:\n",
        "        return json.loads(f.read())\n",
        "\n",
        "from multiprocessing import Pool, Manager\n",
        "manager = Manager()\n",
        "matchDict = manager.dict()\n",
        "def match(src, objs, grade):\n",
        "        for obj in objs:\n",
        "            value = fuzz.partial_ratio(src,obj)\n",
        "            if value > grade:\n",
        "                matchDict[src] = matchDict.get(src, []) + [[value, obj]]\n",
        "                if len(matchDict[src]) > 1:\n",
        "                    print('------------------------------')\n",
        "        print(len(matchDict))\n",
        "\n",
        "def myfuzzymatch(srcs,objs,grade=80):\n",
        "    p = Pool(32)\n",
        "    for src in list(srcs): # [:100]:\n",
        "        p.apply_async(match, args=(src, objs, grade))\n",
        "    p.close()\n",
        "    p.join()\n",
        "\n",
        "    new_match = { k:sorted(v, reverse=True) for k,v in matchDict.items() }\n",
        "\n",
        "    return new_match\n",
        "\n",
        "def fuzz_list(node1_list,node2_list,score_baseline=66,proposal_num=10,string_map=None):\n",
        "    node_dict = { }\n",
        "    for i,node1 in enumerate(node1_list):\n",
        "        match_score_dict = { }\n",
        "        for node2 in node2_list:\n",
        "            if node1 != node2:\n",
        "                if string_map is not None:\n",
        "                    n1 = string_map(node1)\n",
        "                    n2 = string_map(node2)\n",
        "                    score = fuzz.partial_ratio(n1,n2)\n",
        "                    if n1 == n2:\n",
        "                        node2_list.remove(node2)\n",
        "                else:\n",
        "                    score = fuzz.partial_ratio(node1,node2)\n",
        "                if score > score_baseline:\n",
        "                    match_score_dict[node2] = score\n",
        "            else:\n",
        "                node2_list.remove(node2)\n",
        "        node2_sort = sorted(match_score_dict.keys(), key=lambda k:match_score_dict[k],reverse=True)\n",
        "        node_dict[node1] = [[n,match_score_dict[n]] for n in node2_sort[:proposal_num]]\n",
        "        print(i,len(node1_list))\n",
        "    return node_dict, node2_list\n",
        "\n"
      ],
      "metadata": {
        "id": "u5X8anSdER2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The code below contains functions for visualizing the results of the model's predictions, the training process (e.g., loss over epochs), and possibly the attention weights to interpret the model's decision-making process.\n",
        "\n",
        "* This portion contains utility functions for plotting graphs and visualizations.\n",
        "* It defines functions like plot_multi_graph and plot_multi_line to plot multiple images or line graphs in a grid layout."
      ],
      "metadata": {
        "id": "zMS1-FR6sp3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the code below is imported from plot.py\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_multi_graph(image_list, name_list, save_path=None, show=False):\n",
        "    graph_place = int(np.sqrt(len(name_list) - 1)) + 1\n",
        "    print('eeee')\n",
        "    for i, (image, name) in enumerate(zip(image_list, name_list)):\n",
        "        ax1 = plt.subplot(graph_place,graph_place,i+1)\n",
        "        ax1.set_title(name)\n",
        "        # plt.imshow(image,cmap='gray')\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "        print('eeee')\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print('eeee')\n",
        "        pass\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "def plot_multi_line(x_list, y_list, name_list, save_path=None, show=False):\n",
        "    plt.clf()\n",
        "    graph_place = int(np.sqrt(len(name_list) - 1)) + 1\n",
        "    for i, (x, y, name) in enumerate(zip(x_list, y_list, name_list)):\n",
        "        ax1 = plt.subplot(graph_place,graph_place,i+1)\n",
        "        # ax1 = plt.subplot(1,graph_place,i+1)\n",
        "        fontsize= 15\n",
        "        ax1.set_title(name, fontsize=fontsize)\n",
        "        plt.plot(x,y, markersize=12)\n",
        "\n",
        "        plt.xticks([-120, -90, -60, -30, -7])\n",
        "        plt.xlim(-130, 0)\n",
        "        # plt.imshow(image,cmap='gray')\n",
        "        plt.xlabel('Hold-Off Window', fontsize=fontsize)\n",
        "        if i == 0:\n",
        "            plt.ylabel('Comtribution Rate', fontsize=fontsize)\n",
        "            plt.ylim(0.855, 0.915)\n",
        "        else:\n",
        "            plt.ylim(0.085, 0.145)\n",
        "    if save_path:\n",
        "        plt.savefig(save_path+'.eps')\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def plot_multi_line_one_image(x_list, y_list, name, save_path=None, show=False):\n",
        "    plt.clf()\n",
        "    for i, (x, y) in enumerate(zip(x_list, y_list)):\n",
        "        ax1 = plt.subplot(1,1,1)\n",
        "        plt.plot(x,y)\n",
        "        if i == 0:\n",
        "            ax1.set_title(name)\n",
        "            plt.xlabel('Hold-off window')\n",
        "            plt.ylabel('Comtribution rate')\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "    if show:\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "Dln59DOPs3L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Data**\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
        "  * Illustration: printing results, plotting figures for illustration.\n",
        "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset.\n",
        "\n",
        "**Please note that in this draft we will be using a smaller dataset which is derived from the original dataset only for demo, as the original one is too large and will take a couple of hours to process.**"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyhealth"
      ],
      "metadata": {
        "id": "VfHlBU9UPUGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Case-control matching**\n",
        "\n",
        "In this code, we first load the MIMIC-III dataset using the PyHealth library. We define the ICD-9 codes for heart failure and the desired prediction window (120 days in this example).\n",
        "\n",
        "Next, we identify case patients based on the presence of heart failure diagnosis codes. For each case patient, we find control patients with similar age (within a 5-year range) and gender who do not have a heart failure diagnosis before the index date of the case patient. We randomly select a fixed number of control patients (num_controls) for each case patient and assign them the same index date.\n",
        "\n",
        "After matching case and control patients, we combine them into a single dataset. We then extract the EHR data for each patient within the specified prediction window.\n",
        "\n",
        "Finally, we can perform further data preprocessing and feature selection on the matched dataset, such as converting diagnosis codes to a higher-level representation, removing infrequent events, and focusing on diagnosis-related events."
      ],
      "metadata": {
        "id": "J4jBEpcLAnuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import And\n",
        "import pyhealth\n",
        "from pyhealth.datasets import MIMIC3Dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Specify the root directory of the MIMIC-III dataset\n",
        "mimic_root = '/content/drive/MyDrive/Colab_Notebooks/Data/Demo_Data'\n",
        "\n",
        "# Specify the tables to load from the MIMIC-III dataset\n",
        "mimic_tables = ['DIAGNOSES_ICD']\n",
        "\n",
        "# Initialize the MIMIC-III dataset\n",
        "mimic = MIMIC3Dataset(root=mimic_root, tables=mimic_tables)\n",
        "\n",
        "# Define the ICD-9 codes for heart failure (HF)\n",
        "hf_codes = [\n",
        "    '428', '4280', '4281', '4282', '42820', '42821', '42822', '42823',\n",
        "    '4283', '42830', '42831', '42832', '42833', '4284', '42840', '42841',\n",
        "    '42842', '42843', '4289', '40201', '40211', '40291', '40401', '40403',\n",
        "    '40411', '40413', '40491', '40493'\n",
        "]\n",
        "\n",
        "# Define the number of controls per case\n",
        "num_controls = 3\n",
        "\n",
        "# Initialize an empty dictionary to store patients' data\n",
        "patients_data = {}\n",
        "\n",
        "patients_data = mimic.parse_basic_info(patients_data)\n",
        "patients_data = mimic.parse_diagnoses_icd(patients_data)\n",
        "\n",
        "\n",
        "# Create a list to store the case and control patients\n",
        "case_patients = []\n",
        "control_patients = []\n",
        "\n",
        "# print(patients_data[list(patients_data.keys())[1]])\n",
        "\n",
        "# Iterate over each patient in the patients_data dictionary\n",
        "for patient_id, patient_info in patients_data.items():\n",
        "    # Get the patient's visits\n",
        "    visits = patient_info.visits\n",
        "\n",
        "    # Check if the patient has heart failure diagnosis\n",
        "    for visit_id, visit in visits.items():\n",
        "        for diagnosis in visit.get_code_list('DIAGNOSES_ICD'):\n",
        "            if diagnosis in hf_codes:\n",
        "                case_patients.append(patient_id)\n",
        "            else:\n",
        "                control_patients.append(patient_id)\n",
        "\n",
        "max_year = 0\n",
        "\n",
        "for case_patient_id in case_patients:\n",
        "    if patients_data[case_patient_id].death_datetime is not None and patients_data[case_patient_id].death_datetime.year > max_year:\n",
        "        max_year = patients_data[case_patient_id].death_datetime.year\n",
        "\n",
        "\n",
        "# Create a list to store the matched control patients\n",
        "matched_controls = []\n",
        "matched_control_patients = []\n",
        "\n",
        "# Iterate over each case patient\n",
        "for case_patient_id in case_patients:\n",
        "    # Get the case patient's age and gender\n",
        "    if patients_data[case_patient_id].death_datetime is None:\n",
        "        case_age = max_year - patients_data[case_patient_id].birth_datetime.year\n",
        "    else:\n",
        "        case_age = patients_data[case_patient_id].death_datetime.year - patients_data[case_patient_id].birth_datetime.year\n",
        "\n",
        "    case_gender = patients_data[case_patient_id].gender\n",
        "\n",
        "    # Find control patients with similar age and gender\n",
        "    # Temporarily disable the age filter for smaller dataset. This filter will be\n",
        "    # re-enabled in the formal final report.\n",
        "    for patient_id in control_patients:\n",
        "        # if patients_data[patient_id].death_datetime is None:\n",
        "        #     patient_age = max_year - patients_data[patient_id].birth_datetime.year\n",
        "        # else:\n",
        "        #     patient_age = patients_data[patient_id].death_datetime.year - patients_data[patient_id].birth_datetime.year\n",
        "        if (\n",
        "            # patient_age >= case_age - 5 and\n",
        "            # patient_age <= case_age + 5 and\n",
        "            patients_data[patient_id].gender == case_gender\n",
        "        ):\n",
        "            matched_control_patients.append(patient_id)\n",
        "\n",
        "    # Randomly select num_controls control patients\n",
        "    selected_controls = pd.Series(matched_control_patients).sample(n=num_controls, random_state=42)\n",
        "\n",
        "    # Add the selected control patients to the matched controls list\n",
        "    matched_controls.extend(selected_controls)\n",
        "\n",
        "# Combine case patients and matched control patients\n",
        "case_control_patient_ids = case_patients + matched_controls\n",
        "\n",
        "# Create a new dictionary with case-control patients' data\n",
        "case_control_patients_data = {patient_id: patients_data[patient_id] for patient_id in case_control_patient_ids}"
      ],
      "metadata": {
        "id": "59pXn7ZdGxnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Data Extraction**\n",
        "\n",
        "In this code:\n",
        "\n",
        "1. Since we have already performed case-control matching and obtained the matched patient IDs, which are stored in the `case_control_patient_ids` list, we first list all IDs in the `case_control_patient_ids`.\n",
        "2. We initialize an empty dictionary called `extracted_data` to store the extracted data for each matched patient.\n",
        "3. We iterate over each matched patient ID and extract the relevant information:\n",
        "  - Basic patient information (age and gender) is extracted from the patients_data dictionary.\n",
        "  - Diagnoses, procedures, prescriptions, and lab events are extracted from the patient's visits in the `patients_data` dictionary.\n",
        "5. The extracted data for each patient is appended to the corresponding lists in the `extracted_data` dictionary.\n",
        "6. After extracting data for all matched patients, we convert the `extracted_data` dictionary into a pandas DataFrame called `extracted_df`.\n",
        "7. Finally, we save the extracted data to a file (e.g., CSV) using the `to_csv()` method of the `DataFrame`."
      ],
      "metadata": {
        "id": "uZkiK6GqFqry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty dictionary to store the extracted data\n",
        "extracted_data = {\n",
        "    'patient_id': [],\n",
        "    'age': [],\n",
        "    'gender': [],\n",
        "    'diagnoses': [],\n",
        "    'is_case': []\n",
        "}\n",
        "\n",
        "\n",
        "# Iterate over each matched patient ID\n",
        "for patient_id in case_control_patient_ids:\n",
        "    # Get the patient's basic information\n",
        "    patient_info = patients_data[patient_id]\n",
        "\n",
        "    # Extract age and gender\n",
        "    if patient_info.death_datetime is None:\n",
        "        age = max_year - patient_info.birth_datetime.year\n",
        "    else:\n",
        "        age = patient_info.death_datetime.year - patient_info.birth_datetime.year\n",
        "    gender = patient_info.gender\n",
        "\n",
        "    is_case = []\n",
        "\n",
        "    # Extract diagnoses\n",
        "    diagnoses = []\n",
        "    for visit_id, visit in patient_info.visits.items():\n",
        "        for diagnosis in visit.get_code_list('DIAGNOSES_ICD'):\n",
        "            if diagnosis in hf_codes:\n",
        "                is_case.append('1')\n",
        "            else:\n",
        "                is_case.append('0')\n",
        "            diagnoses.append(diagnosis)\n",
        "\n",
        "    # Append the extracted data to the dictionary\n",
        "    extracted_data['patient_id'].append(patient_id)\n",
        "    extracted_data['age'].append(age)\n",
        "    extracted_data['gender'].append(gender)\n",
        "    extracted_data['diagnoses'].append(diagnoses)\n",
        "    extracted_data['is_case'].append(is_case[0])\n",
        "\n",
        "# Convert the extracted data into a pandas DataFrame\n",
        "extracted_df = pd.DataFrame(extracted_data)\n",
        "\n",
        "# Perform further data preprocessing, feature engineering, and analysis on the extracted DataFrame\n",
        "# ...\n",
        "\n",
        "# Save the extracted data to a file if needed\n",
        "extracted_df.to_json('/content/drive/MyDrive/Colab_Notebooks/Data/Pre-Processed/extracted_data.json', orient='records')"
      ],
      "metadata": {
        "id": "AKCTO6TtFqUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Feature Selection**\n",
        "\n",
        "In this code:\n",
        "\n",
        "1. We define the feature columns and the target variable. In this example, we assume that the target variable is a binary variable indicating whether a patient is a case (1) or control (0), and it is stored in the `is_case` column of the DataFrame.\n",
        "2. We convert the target variable to binary labels using `astype(int)`.\n",
        "3. We perform one-hot encoding for categorical features (`gender`, `diagnoses`, `procedures`, `prescriptions`, `lab_events`) using `pd.get_dummies()`. This creates binary columns for each unique category in these features.\n",
        "4. We combine the encoded categorical features with the numerical feature (`age`) using `pd.concat()` to create the feature matrix `X`. The target variable `y` is extracted separately.\n",
        "5. We perform feature selection using the `SelectKBest` class from scikit-learn with the chi-square test as the scoring function. We specify the desired number of top features to select using the `k` parameter.\n",
        "6. We fit and transform the feature matrix `X` using the `fit_transform()` method of the `SelectKBest` object, which selects the top `k` features based on their chi-square test scores.\n",
        "7. We retrieve the names of the selected features using the `get_support()` method of the `SelectKBest` object and the column names of `X`.\n",
        "8. We create a new `DataFrame` `selected_df` with the selected features and the target variable using `pd.concat()`.\n",
        "9. Finally, we save the selected features and target variable to a file (e.g., CSV) using the `to_csv()` method of the DataFrame."
      ],
      "metadata": {
        "id": "NBBrrMJwG8N_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Define the feature columns and target variable\n",
        "feature_columns = ['age', 'gender', 'diagnoses']\n",
        "target_variable = 'is_case'\n",
        "\n",
        "# Convert the target variable to binary labels\n",
        "extracted_df[target_variable] = extracted_df[target_variable].astype(int)\n",
        "\n",
        "# Convert list columns to string representations\n",
        "extracted_df['diagnoses'] = extracted_df['diagnoses'].apply(lambda x: ','.join(map(str, x)))\n",
        "\n",
        "# Perform one-hot encoding for categorical features\n",
        "categorical_features = ['gender', 'diagnoses']\n",
        "encoded_features = pd.get_dummies(extracted_df[categorical_features], prefix_sep='_')\n",
        "\n",
        "# Combine the encoded features with the numerical features\n",
        "X = pd.concat([extracted_df[['age']], encoded_features], axis=1)\n",
        "y = extracted_df[target_variable]\n",
        "\n",
        "# Perform feature selection using SelectKBest with chi-square test\n",
        "k = 'all'  # Select all available features\n",
        "selector = SelectKBest(score_func=chi2, k=k)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Get the selected feature names\n",
        "selected_feature_names = X.columns[selector.get_support()].tolist()\n",
        "\n",
        "# Create a new DataFrame with the selected features and target variable\n",
        "selected_df = pd.concat([pd.DataFrame(X_selected, columns=selected_feature_names), y], axis=1)\n",
        "\n",
        "# Perform further data preprocessing, model training, and evaluation on the selected features\n",
        "# ...\n",
        "\n",
        "# Save the selected features and target variable to a file if needed\n",
        "selected_df.to_json('/content/drive/MyDrive/Colab_Notebooks/Data/Pre-Processed/selected_features.json', orient='records')"
      ],
      "metadata": {
        "id": "QTcB32HyG9iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Data Representation**\n",
        "\n",
        "In this code:\n",
        "\n",
        "1. We separate the features (`X`) and the target variable (`y`) from the `selected_df` DataFrame.\n",
        "2. We convert the data to PyTorch tensors using `torch.tensor()`. The features are converted to float32 tensors, and the target variable is converted to float32 tensors and reshaped to have an additional dimension using `unsqueeze(1)`.\n",
        "3. We define a data representation model (`DataRepresentationModel`) using PyTorch's `nn.Module`. This model consists of two fully connected layers with a ReLU activation function in between. The model takes the input features and learns a lower-dimensional representation.\n",
        "4. We set the model parameters, including the input dimension (`input_dim`), hidden dimension (`hidden_dim`), and output dimension (`output_dim`). You can adjust these values based on your specific requirements.\n",
        "5. We create an instance of the data representation model using the specified parameters.\n",
        "6. We define the loss function (`nn.MSELoss()`) and the optimizer (`torch.optim.Adam`) for training the model.\n",
        "7. We set the number of training epochs (`num_epochs`) and start the training loop.\n",
        "8. Inside the training loop, we perform a forward pass to obtain the model outputs, calculate the loss, perform a backward pass to compute gradients, and update the model parameters using the optimizer.\n",
        "9. We print the loss every 10 epochs to monitor the training progress.\n",
        "10. After training, we obtain the learned data representation by passing the input features through the trained model and detaching the output from the computation graph using `detach()`.\n",
        "11. We create a new DataFrame (`representation_df`) with the learned data representation, where each column represents a learned feature.\n",
        "12. Finally, we save the learned data representation to a file (e.g., CSV) using the `to_csv()` method of the DataFrame."
      ],
      "metadata": {
        "id": "DQ7W8AycG-5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Separate features and target variable\n",
        "X = selected_df.drop('is_case', axis=1)\n",
        "y = selected_df['is_case']\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Define the data representation model\n",
        "class DataRepresentationModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(DataRepresentationModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Set the model parameters\n",
        "input_dim = X_tensor.shape[1]\n",
        "hidden_dim = 100\n",
        "output_dim = 50\n",
        "\n",
        "# Create an instance of the data representation model\n",
        "model = DataRepresentationModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Set the number of training epochs\n",
        "num_epochs = 100\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_tensor)\n",
        "    loss = criterion(outputs, y_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss for every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Get the learned data representation\n",
        "data_representation = model(X_tensor).detach().numpy()\n",
        "\n",
        "# Create a new DataFrame with the learned data representation and target variable\n",
        "representation_df = pd.DataFrame(data_representation, columns=[f'feature_{i}' for i in range(output_dim)])\n",
        "representation_df['is_case'] = y\n",
        "\n",
        "\n",
        "# Perform further analysis or use the learned data representation for downstream tasks\n",
        "# ...\n",
        "\n",
        "# Save the learned data representation to a file if needed\n",
        "representation_df.to_json('/content/drive/MyDrive/Colab_Notebooks/Data/Pre-Processed/data_representation.json', orient='records')"
      ],
      "metadata": {
        "id": "1j5H4b2kG-wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Data Split**\n",
        "\n",
        "In this code:\n",
        "\n",
        "1. We separate the features (`X`) and the target variable (`y`) from the `final_df` DataFrame.\n",
        "2. We split the data into training, validation, and testing sets using the `train_test_split()` function from `scikit-learn`. We first split the data into training and temporary sets with a test size of 0.3 (i.e., 30% of the data). Then, we further split the temporary set into validation and testing sets with a test size of 0.5 (i.e., 50% of the temporary set). We use stratified splitting by specifying `stratify=y` to ensure that the class distribution is maintained in each split.\n",
        "3. We convert the split data to PyTorch tensors using `torch.tensor()`. The features are converted to `float32` tensors, and the target variable is converted to `float32` tensors and reshaped to have an additional dimension using `unsqueeze(1)`.\n",
        "4. We create PyTorch datasets using `torch.utils.data.TensorDataset()` for the training, validation, and testing splits. Each dataset is created by passing the corresponding feature and target tensors.\n",
        "5. We create PyTorch dataloaders using `torch.utils.data.DataLoader()` for the training, validation, and testing datasets. The dataloaders allow for efficient batch-wise data loading and shuffling during training. We can specify the batch size and whether to shuffle the data using the `batch_size` and shuffle parameters.\n",
        "6. Finally, we save the split datasets to files using `torch.save()`. This allows us to load the datasets directly in the future without repeating the splitting process."
      ],
      "metadata": {
        "id": "tSvBo2JE5lQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you have already performed case-control matching, data extraction, feature selection, data representation, and obtained the final DataFrame\n",
        "final_df = representation_df  # Final DataFrame after case-control matching, data extraction, feature selection, and data representation\n",
        "\n",
        "# Separate features and target variable\n",
        "X = final_df.drop('is_case', axis=1)\n",
        "y = final_df['is_case']\n",
        "\n",
        "# This section could probably be removed with larger dataset.\n",
        "# Remove samples with the minority class\n",
        "minority_class = y.value_counts().idxmin()\n",
        "mask = y != minority_class\n",
        "X = X[mask]\n",
        "y = y[mask]\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "# Create DataFrames for each split\n",
        "train_df = pd.concat([X_train, y_train], axis=1)\n",
        "val_df = pd.concat([X_val, y_val], axis=1)\n",
        "test_df = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "# Save the split datasets to JSON files\n",
        "train_df.to_json('/content/drive/MyDrive/Colab_Notebooks/Data/train.json', orient='records')\n",
        "val_df.to_json('/content/drive/MyDrive/Colab_Notebooks/Data/valid.json', orient='records')\n",
        "test_df.to_json('/content/drive/MyDrive/Colab_Notebooks/Data/test.json', orient='records')"
      ],
      "metadata": {
        "id": "Cio77nvP5lHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### KnowLife Preprocessing"
      ],
      "metadata": {
        "id": "tfkIbPa9bEPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import pyhealth\n",
        "import string\n",
        "from pyhealth.medcode import CrossMap\n",
        "\n",
        "process_graph = False\n",
        "\n",
        "if process_graph:\n",
        "\n",
        "    knowlife_path = '/content/drive/MyDrive/Colab_Notebooks/Data/KnowLife/'\n",
        "    input_file = 'knowlife.csv'\n",
        "    output_file = 'graph.txt'\n",
        "\n",
        "    # UMLS to ICD9CM tools\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    mapping = CrossMap(\"UMLS\", \"ICD9CM\")\n",
        "\n",
        "    # Read file and filter out rows where relation != causes and correct != t\n",
        "    df = pd.read_csv(knowlife_path + input_file, header=0, names=['head','relation','tail','correct'])\n",
        "    df = df.loc[(df['correct']=='t') & (df['relation']=='causes')].drop(columns=['correct'])\n",
        "\n",
        "    # Map UMLS to ICD9CM\n",
        "    df['head'] = df['head'].apply(lambda c: mapping.map(c))\n",
        "    df['tail'] = df['tail'].apply(lambda c: mapping.map(c))\n",
        "\n",
        "    # Turn many-to-many mappings to one-to-one mappings\n",
        "    df = df.explode('head').explode('tail')\n",
        "\n",
        "    # Drop rows without mapping\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Remove periods/punctuation from ICD9CM\n",
        "    df['head'] = df['head'].apply(lambda c: c.translate(translator))\n",
        "    df['tail'] = df['tail'].apply(lambda c: c.translate(translator))\n",
        "\n",
        "    # Create is-caused-by graph and concatenate with causes graph\n",
        "    df_reverse = df.copy()\n",
        "    df_reverse['head'], df_reverse['tail'] = df_reverse['tail'], df_reverse['head']\n",
        "    df_reverse['relation'] = 'is-caused-by'\n",
        "    df = pd.concat([df,df_reverse]).drop_duplicates()\n",
        "\n",
        "    # Write to drive\n",
        "    df.to_csv(knowlife_path + output_file, sep='\\t', header=False, index=False)"
      ],
      "metadata": {
        "id": "l-lCmq5OFTw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####KnowLife Embedding Generation"
      ],
      "metadata": {
        "id": "sh2UZsDibXpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taken from https://github.com/sunlabuiuc/PyHealth/blob/4febba6061253c4c178fa035d2afa67e206fc9a8/pyhealth/medcode/pretrained_embeddings/kg_emb/examples/train_kge_model.py\n",
        "from pyhealth.medcode.pretrained_embeddings.kg_emb.datasets import UMLSDataset, split\n",
        "from pyhealth.medcode.pretrained_embeddings.kg_emb.tasks import link_prediction_fn\n",
        "from pyhealth.datasets import get_dataloader\n",
        "from pyhealth.medcode.pretrained_embeddings.kg_emb.models import TransE\n",
        "from pyhealth.trainer import Trainer\n",
        "from pyhealth.medcode import InnerMap\n",
        "import torch\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "generate_embedding = False\n",
        "load_best_checkpoint = True\n",
        "load_last_checkpoint = False\n",
        "\n",
        "if generate_embedding:\n",
        "\n",
        "    knowlife_path = '/content/drive/MyDrive/Colab_Notebooks/Data/KnowLife/'\n",
        "    experiment = 'exp1'\n",
        "\n",
        "    # dataset is KnowLife even though we used UMLSDataset class\n",
        "    ds = UMLSDataset(root=knowlife_path, dev=False)\n",
        "\n",
        "    # check the dataset statistics before setting task\n",
        "    print(ds.stat())\n",
        "\n",
        "    # check the relation numbers in the dataset\n",
        "    print(\"Relations in KG:\", ds.relation2id)\n",
        "\n",
        "    ds = ds.set_task(link_prediction_fn, negative_sampling=64, save=False)\n",
        "\n",
        "    # check the dataset statistics after setting task\n",
        "    print(ds.stat())\n",
        "\n",
        "    # split the dataset and get the dataloaders\n",
        "    train_dataset, val_dataset, test_dataset = split(ds, [0.8, 0.1, 0.1])\n",
        "    train_loader = get_dataloader(train_dataset, batch_size=8, shuffle=True)\n",
        "    val_loader = get_dataloader(val_dataset, batch_size=2, shuffle=False)\n",
        "    test_loader = get_dataloader(test_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "    # initialize TransE model\n",
        "    model = TransE(dataset=ds, e_dim=512, r_dim=512)\n",
        "\n",
        "    if load_best_checkpoint:\n",
        "        # load best checkpoint\n",
        "        state_dict = torch.load(knowlife_path + experiment + \"/best.ckpt\")\n",
        "        model.load_state_dict(state_dict)\n",
        "        print('Loaded best checkpoint: ', model)\n",
        "    else:\n",
        "        # load last checkpoint\n",
        "        if load_last_checkpoint:\n",
        "            state_dict = torch.load(knowlife_path + experiment + \"/last.ckpt\")\n",
        "            model.load_state_dict(state_dict)\n",
        "            print('Loaded last checkpoint: ', model)\n",
        "\n",
        "        # train model\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            device=\"cuda\",\n",
        "            metrics=['hits@n', 'mean_rank'],\n",
        "            output_path=knowlife_path,\n",
        "            exp_name=experiment\n",
        "        )\n",
        "\n",
        "        trainer.train(\n",
        "            train_dataloader=train_loader,\n",
        "            val_dataloader=val_loader,\n",
        "            epochs=200,\n",
        "            optimizer_params={'lr': 1e-3},\n",
        "            monitor='mean_rank',\n",
        "            monitor_criterion='min'\n",
        "        )\n",
        "\n",
        "        # evaluate model\n",
        "        trainer.evaluate(test_loader)\n",
        "\n",
        "    # save entity2id and relation2id\n",
        "    with open(knowlife_path + experiment + \"/entity2id.json\", \"w\") as f:\n",
        "        json.dump(ds.entity2id, f, indent=4)\n",
        "    with open(knowlife_path + experiment + \"/relation2id.json\", \"w\") as f:\n",
        "        json.dump(ds.relation2id, f, indent=4)\n",
        "\n",
        "    # save entity and relation embeddings\n",
        "    torch.save(model.E_emb, knowlife_path + experiment + \"/entity_embeddings.pt\")\n",
        "    torch.save(model.R_emb, knowlife_path + experiment + \"/relation_embeddings.pt\")\n"
      ],
      "metadata": {
        "id": "IuttkBtcbX9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### UMLS Preprocessing"
      ],
      "metadata": {
        "id": "rx1B_CDabfAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import pyhealth\n",
        "import string\n",
        "from pyhealth.medcode import CrossMap\n",
        "\n",
        "process_graph = False\n",
        "\n",
        "if process_graph:\n",
        "\n",
        "    umls_path = '/content/drive/MyDrive/Colab_Notebooks/Data/UMLS/'\n",
        "    input_file = 'MRREL.RRF.zip'\n",
        "    output_file = 'graph.txt'\n",
        "\n",
        "    # UMLS to ICD9CM tools\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    mapping = CrossMap(\"UMLS\", \"ICD9CM\")\n",
        "\n",
        "    # Read file and filter out rows where relation != cause_of\n",
        "    df = pd.read_csv(umls_path + input_file, sep='|', header=None, usecols=[0,4,7], dtype='string')\n",
        "    df.columns = ['tail','head','relation']\n",
        "    df = df[['head','relation','tail']]\n",
        "    df = df.loc[df['relation'] == 'cause_of']\n",
        "    df['relation'] = 'causes'\n",
        "\n",
        "    # Map UMLS to ICD9CM\n",
        "    df['head'] = df['head'].apply(lambda c: mapping.map(c))\n",
        "    df['tail'] = df['tail'].apply(lambda c: mapping.map(c))\n",
        "\n",
        "    # Turn many-to-many mappings to one-to-one mappings\n",
        "    df = df.explode('head').explode('tail')\n",
        "\n",
        "    # Drop rows without mapping\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Remove periods/punctuation from ICD9CM\n",
        "    df['head'] = df['head'].apply(lambda c: c.translate(translator))\n",
        "    df['tail'] = df['tail'].apply(lambda c: c.translate(translator))\n",
        "\n",
        "    # Create is-caused-by graph and concatenate with causes graph\n",
        "    df_reverse = df.copy()\n",
        "    df_reverse['head'], df_reverse['tail'] = df_reverse['tail'], df_reverse['head']\n",
        "    df_reverse['relation'] = 'is-caused-by'\n",
        "    df = pd.concat([df,df_reverse]).drop_duplicates()\n",
        "\n",
        "    # Write to drive\n",
        "    df.to_csv(umls_path + output_file, sep='\\t', header=False, index=False)"
      ],
      "metadata": {
        "id": "bfSyP0ySbfIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####UMLS Embedding Generation"
      ],
      "metadata": {
        "id": "UercsbQ_bmjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taken from https://github.com/sunlabuiuc/PyHealth/blob/4febba6061253c4c178fa035d2afa67e206fc9a8/pyhealth/medcode/pretrained_embeddings/kg_emb/examples/train_kge_model.py\n",
        "from pyhealth.medcode.pretrained_embeddings.kg_emb.datasets import UMLSDataset, split\n",
        "from pyhealth.medcode.pretrained_embeddings.kg_emb.tasks import link_prediction_fn\n",
        "from pyhealth.datasets import get_dataloader\n",
        "from pyhealth.medcode.pretrained_embeddings.kg_emb.models import TransE\n",
        "from pyhealth.trainer import Trainer\n",
        "from pyhealth.medcode import InnerMap\n",
        "import torch\n",
        "import json\n",
        "\n",
        "generate_embedding = False\n",
        "load_best_checkpoint = True\n",
        "load_last_checkpoint = False\n",
        "\n",
        "if generate_embedding:\n",
        "\n",
        "    umls_path = '/content/drive/MyDrive/Colab_Notebooks/Data/UMLS/'\n",
        "    experiment = 'exp1'\n",
        "\n",
        "    # dataset is KnowLife even though we used UMLSDataset class\n",
        "    ds = UMLSDataset(root=umls_path, dev=False)\n",
        "\n",
        "    # check the dataset statistics before setting task\n",
        "    print(ds.stat())\n",
        "\n",
        "    # check the relation numbers in the dataset\n",
        "    print(\"Relations in KG:\", ds.relation2id)\n",
        "\n",
        "    ds = ds.set_task(link_prediction_fn, negative_sampling=64, save=False)\n",
        "\n",
        "    # check the dataset statistics after setting task\n",
        "    print(ds.stat())\n",
        "\n",
        "    # split the dataset and get the dataloaders\n",
        "    train_dataset, val_dataset, test_dataset = split(ds, [0.8, 0.1, 0.1])\n",
        "    train_loader = get_dataloader(train_dataset, batch_size=8, shuffle=True)\n",
        "    val_loader = get_dataloader(val_dataset, batch_size=2, shuffle=False)\n",
        "    test_loader = get_dataloader(test_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "    # initialize TransE model\n",
        "    model = TransE(dataset=ds, e_dim=512, r_dim=512)\n",
        "\n",
        "    if load_best_checkpoint:\n",
        "        # load best checkpoint\n",
        "        state_dict = torch.load(umls_path + experiment + \"/best.ckpt\")\n",
        "        model.load_state_dict(state_dict)\n",
        "        print('Loaded best checkpoint: ', model)\n",
        "    else:\n",
        "        # load checkpoint\n",
        "        if load_last_checkpoint:\n",
        "            state_dict = torch.load(umls_path + experiment + \"/last.ckpt\")\n",
        "            model.load_state_dict(state_dict)\n",
        "            print('Loaded last checkpoint: ', model)\n",
        "\n",
        "        # train model\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            device=\"cuda\",\n",
        "            metrics=['hits@n', 'mean_rank'],\n",
        "            output_path=umls_path,\n",
        "            exp_name=experiment\n",
        "        )\n",
        "\n",
        "        trainer.train(\n",
        "            train_dataloader=train_loader,\n",
        "            val_dataloader=val_loader,\n",
        "            epochs=400,\n",
        "            optimizer_params={'lr': 1e-3},\n",
        "            monitor='mean_rank',\n",
        "            monitor_criterion='min'\n",
        "        )\n",
        "\n",
        "        # evaluate model\n",
        "        trainer.evaluate(test_loader)\n",
        "\n",
        "    # save entity2id and relation2id\n",
        "    with open(umls_path + experiment + \"/entity2id.json\", \"w\") as f:\n",
        "        json.dump(ds.entity2id, f, indent=4)\n",
        "    with open(umls_path + experiment + \"/relation2id.json\", \"w\") as f:\n",
        "        json.dump(ds.relation2id, f, indent=4)\n",
        "\n",
        "    # save entity and relation embeddings\n",
        "    torch.save(model.E_emb, umls_path + experiment + \"/entity_embeddings.pt\")\n",
        "    torch.save(model.R_emb, umls_path + experiment + \"/relation_embeddings.pt\")\n"
      ],
      "metadata": {
        "id": "FX1QPr8ZbmsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "62GzcbNccoX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pyhealth\n",
        "from pyhealth.datasets import MIMIC3Dataset\n",
        "from collections import defaultdict\n",
        "from pyhealth.tasks import mortality_prediction_mimic3_fn\n",
        "\n",
        "# Specify the root directory of the MIMIC-III dataset\n",
        "mimic_root = '/content/drive/MyDrive/Colab_Notebooks/Data/MIMIC-III'\n",
        "\n",
        "# Initialize the MIMIC-III dataset\n",
        "mimic = MIMIC3Dataset(root=mimic_root, tables=mimic_tables)\n",
        "\n",
        "\n",
        "# build icd2idx\n",
        "icds = defaultdict(int)\n",
        "for patient in mimic.patients.values():\n",
        "    for visit in patient.visits.values():\n",
        "        for code in visit.get_code_list(table='DIAGNOSES_ICD'):\n",
        "            icds[code] += 1\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/Data/UMLS/entity2id.json', 'r') as f:\n",
        "    icd2idx = json.load(f)\n",
        "\n",
        "idx = len(icd2idx)\n",
        "for code, freq in icds.items():\n",
        "    # ignore codes that appear less than 10 times\n",
        "    if code not in icd2idx and freq >= 10:\n",
        "        icd2idx[code] = idx\n",
        "        idx += 1\n",
        "idx2icd = {id: c for c, id in icd2idx.items()}\n",
        "\n",
        "# need to do case control matching\n",
        "total_case, total_control = 0, 0\n",
        "max_case, max_control = 425, 1275\n",
        "\n",
        "# define heart failure task\n",
        "def heart_failure_task(patient):\n",
        "\n",
        "    global total_case\n",
        "    global total_control\n",
        "\n",
        "    if total_case == max_case and total_control == max_control:\n",
        "        return []\n",
        "\n",
        "    samples, conditions, masks = [], [], []\n",
        "    min_length, max_length = 10, 20\n",
        "    heart_failure = 0\n",
        "\n",
        "    for visit in patient.visits.values():\n",
        "        for code in visit.get_code_list(table='DIAGNOSES_ICD'):\n",
        "            if code in hf_codes:\n",
        "                heart_failure = 1\n",
        "                break\n",
        "            elif code in icd2idx:\n",
        "                # need to add timestamp later\n",
        "                conditions.append(icd2idx[code])\n",
        "                masks.append(1)\n",
        "\n",
        "        if heart_failure:\n",
        "            break\n",
        "\n",
        "\n",
        "    # select cases with 10 to 20 medical codes\n",
        "    if len(conditions) >= min_length:\n",
        "        if len(conditions) > max_length:\n",
        "            conditions = conditions[-max_length:]\n",
        "            masks = masks[-max_length:]\n",
        "        else:\n",
        "            conditions += [0] * (max_length - len(conditions))\n",
        "            masks += [0] * (max_length - len(masks))\n",
        "\n",
        "        if heart_failure and total_case < max_case:\n",
        "            # new case\n",
        "            samples.append(\n",
        "                {\n",
        "                    \"visit_id\": visit.visit_id,\n",
        "                    \"patient_id\": patient.patient_id,\n",
        "                    \"conditions\": conditions,\n",
        "                    \"masks\": masks,\n",
        "                    \"label\": heart_failure,\n",
        "                }\n",
        "            )\n",
        "            total_case += 1\n",
        "        elif not heart_failure and total_control < max_control:\n",
        "            # new control\n",
        "            samples.append(\n",
        "                {\n",
        "                    \"visit_id\": visit.visit_id,\n",
        "                    \"patient_id\": patient.patient_id,\n",
        "                    \"conditions\": conditions,\n",
        "                    \"masks\": masks,\n",
        "                    \"label\": heart_failure,\n",
        "                }\n",
        "            )\n",
        "            total_control += 1\n",
        "\n",
        "    return samples\n",
        "\n",
        "\n",
        "mimic_samples = mimic.set_task(heart_failure_task)\n",
        "mimic_samples.stat()"
      ],
      "metadata": {
        "id": "uCmDiqCbcnOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyhealth.datasets.splitter import split_by_sample\n",
        "from pyhealth.datasets import get_dataloader\n",
        "# split into train, val, test\n",
        "train_dataset, val_dataset, test_dataset = split_by_sample(mimic_samples, [0.8, 0.1, 0.1])\n",
        "\n",
        "# create dataloaders\n",
        "train_loader = get_dataloader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = get_dataloader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = get_dataloader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "NStOC91d0d54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   **Model**\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "1, Model architecture:\n",
        "* The model portion is imported mainly from original model.py file. The model is defined in the FCModel class, which inherits from nn.Module.\n",
        "* The model consists of an embedding layer  to map medical events to dense vectors.\n",
        "* It uses an LSTM (LSTMCore) as the core component to process the sequence of embedded medical events.\n",
        "* The LSTM core has a configurable hidden size\n",
        "* The model includes attention mechanisms:\n",
        "Knowledge graph attention to attend to relevant entities in the knowledge graph.\n",
        "\n",
        "The model uses fully connected linear layers (nn.Linear) to transform the LSTM outputs and generate predictions.\n",
        "Activation functions used include ReLU (F.relu) and sigmoid (nn.Sigmoid).\n",
        "\n",
        "2, Training objectives:\n",
        "* The loss function is binary cross-entropy loss (nn.BCELoss)\n",
        "\n",
        "3, Others:\n",
        "* The model uses pre-trained embeddings for medical entities and relations, loaded from entity_embeddings.pt and relation_embeddings.pt"
      ],
      "metadata": {
        "id": "k_9lPPA4t_LA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# below imported from model.py\n",
        "\n",
        "def _cuda(tensor):\n",
        "    if args.gpu:\n",
        "        return tensor.cuda(non_blocking=True) # revised code\n",
        "    else:\n",
        "        return tensor\n",
        "\n",
        "class LSTMCore(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout):\n",
        "        super(LSTMCore, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Build a LSTM\n",
        "        self.i2h = nn.Linear(self.input_size, 4 * self.hidden_size)\n",
        "        self.h2h = nn.Linear(self.hidden_size, 4 * self.hidden_size)\n",
        "\n",
        "    def forward(self, xt, state):\n",
        "        # add time encodings\n",
        "\n",
        "        prev_h, prev_c = state\n",
        "\n",
        "        all_input_sums = self.i2h(xt) + self.h2h(prev_h)\n",
        "        sigmoid_chunk = all_input_sums.narrow(1, 0, 3 * self.hidden_size)\n",
        "        sigmoid_chunk = F.sigmoid(sigmoid_chunk)\n",
        "        in_gate = sigmoid_chunk.narrow(1, 0, self.hidden_size)\n",
        "        forget_gate = sigmoid_chunk.narrow(1, self.hidden_size, self.hidden_size)\n",
        "        out_gate = sigmoid_chunk.narrow(1, self.hidden_size * 2, self.hidden_size)\n",
        "\n",
        "        in_transform = all_input_sums.narrow(1, 3 * self.hidden_size, self.hidden_size)\n",
        "        next_c = in_transform * in_gate + forget_gate * prev_c\n",
        "        next_h = out_gate * F.tanh(next_c)\n",
        "\n",
        "        state = (next_h, next_c)\n",
        "        output = next_h\n",
        "\n",
        "        return output, state\n",
        "\n",
        "class FCModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FCModel, self).__init__()\n",
        "        ### === settings ===\n",
        "        self.graph_data_path = '/content/drive/MyDrive/Colab_Notebooks/Data/UMLS/'\n",
        "        self.load_pretrained_embeddings = True\n",
        "        self.use_knowledge_graph = True\n",
        "        self.use_graph_attention = True\n",
        "        self.use_global_pooling = True\n",
        "\n",
        "        self.embedding_size = 512\n",
        "        self.rnn_hidden_size = 512\n",
        "        self.rnn_dropout = 0.5\n",
        "        self.vocab = icd2idx\n",
        "\n",
        "        self.core = LSTMCore(self.embedding_size, self.rnn_hidden_size, self.rnn_dropout)\n",
        "        self.embed = nn.Embedding(len(self.vocab) , self.embedding_size)\n",
        "        self.logit = nn.Linear(self.rnn_hidden_size, 1, False)\n",
        "\n",
        "        ### === knowledge graph ===\n",
        "\n",
        "        # load entities\n",
        "        with open(self.graph_data_path + \"entity2id.json\", \"r\") as f:\n",
        "            self.entity2id = json.load(f)\n",
        "        self.id2entity = {id: e for e, id in self.entity2id.items()}\n",
        "\n",
        "        # load relations\n",
        "        with open(self.graph_data_path + \"relation2id.json\", \"r\") as f:\n",
        "            self.relation2id = json.load(f)\n",
        "        self.id2relation = {id: r for r, id in self.relation2id.items()}\n",
        "\n",
        "        # load graph\n",
        "        self.graph = defaultdict(list)\n",
        "        with open(self.graph_data_path + \"graph.txt\", \"r\") as f:\n",
        "            csv_f = csv.reader(f, delimiter='\\t')\n",
        "            for line in csv_f:\n",
        "                head, relation, tail = line\n",
        "                self.graph[self.entity2id[head]].append([self.entity2id[head],self.entity2id[tail],self.relation2id[relation]])\n",
        "\n",
        "        # initialize graph parameters\n",
        "        self.wr = nn.Linear(self.embedding_size, self.embedding_size)\n",
        "        self.wh = nn.Linear(self.embedding_size, self.embedding_size)\n",
        "        self.wa = nn.Linear(self.embedding_size, self.embedding_size)\n",
        "        self.wt = nn.Linear(self.embedding_size, self.embedding_size)\n",
        "        self.ent_embeddings = nn.Embedding(len(self.entity2id), self.embedding_size)\n",
        "        self.rel_embeddings = nn.Embedding(len(self.relation2id), self.embedding_size)\n",
        "\n",
        "        ### === initialize weights ===\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embed.weight.data.uniform_(-initrange, initrange)\n",
        "        self.logit.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        # initialize embedding weights\n",
        "        if self.load_pretrained_embeddings:\n",
        "            self.ent_embeddings = nn.Embedding.from_pretrained(torch.load(self.graph_data_path + \"entity_embeddings.pt\"))\n",
        "            self.rel_embeddings = nn.Embedding.from_pretrained(torch.load(self.graph_data_path + \"relation_embeddings.pt\"))\n",
        "        else:\n",
        "            self.ent_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "            self.rel_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def kg_attention(self, index, c):\n",
        "\n",
        "        relations = self.graph[index]\n",
        "\n",
        "        if len(relations) > 0:\n",
        "            if self.use_graph_attention:\n",
        "                # dynamic attention\n",
        "                head_embeddings = self.embed(relations[:,0])\n",
        "                tail_embeddings = self.ent_embeddings(relations[:,1])\n",
        "                relation_embeddings = self.rel_embeddings(relations[:,2])\n",
        "\n",
        "                beta = torch.sum(self.wr(relations) * (F.tanh(self.wh(head_embeddings) + self.wa(c) + self.wt(tail_embeddings))), dim=1)\n",
        "                alpha = F.softmax(beta)\n",
        "                gt = torch.mm(alpha, tail_embeddings)\n",
        "\n",
        "            else:\n",
        "                # static\n",
        "                gt = torch.mean(self.ent_embeddings(relations[:,1]), dim=0)\n",
        "\n",
        "        else:\n",
        "            # not present in graph\n",
        "            gt = torch.zeros((self.embedding_size), dtype=torch.float32)\n",
        "\n",
        "        return gt\n",
        "\n",
        "\n",
        "    def forward(self, input, mask, label=None):\n",
        "\n",
        "        # initialize\n",
        "        batch_size = input.size(0)\n",
        "        seq_length = input.size(1)\n",
        "        state = (\n",
        "            torch.zeros((batch_size,self.embedding_size), dtype=torch.float32).to(device),\n",
        "            torch.zeros((batch_size,self.embedding_size), dtype=torch.float32).to(device)\n",
        "        )\n",
        "        outputs = []\n",
        "        outputs_kg = []\n",
        "\n",
        "        # run dg-rnn\n",
        "        for i in range(seq_length):\n",
        "            it = input[:, i]\n",
        "            xt = self.embed(it)\n",
        "\n",
        "            out, state = self.core(xt, state)\n",
        "            outputs.append(out)\n",
        "\n",
        "            # run dynamic attention on knowledge graph\n",
        "            if self.use_knowledge_graph:\n",
        "                h, c = state\n",
        "                gt = []\n",
        "                for j, code in enumerate(it):\n",
        "                    gt.append(self.kg_attention(code, c[j]))\n",
        "                gt = torch.stack(gt).to(device)\n",
        "\n",
        "                out, state = self.core(gt, state)\n",
        "                outputs_kg.append(out)\n",
        "\n",
        "        outputs = torch.stack(outputs,dim=1)\n",
        "        if self.use_knowledge_graph:\n",
        "            outputs_kg = torch.stack(outputs_kg,dim=1)\n",
        "\n",
        "        # collapse with mask\n",
        "        mask = mask.unsqueeze(-1).expand(batch_size, seq_length, self.embedding_size)\n",
        "        if self.use_knowledge_graph:\n",
        "            outputs_kg = outputs_kg * mask\n",
        "            if self.use_global_pooling:\n",
        "                outputs_kg = outputs_kg.max(dim=1)[0]\n",
        "            else:\n",
        "                outputs_kg = outputs_kg[:, -1, :]\n",
        "        else:\n",
        "            outputs = outputs * mask\n",
        "            if self.use_global_pooling:\n",
        "                outputs = outputs.max(dim=1)[0]\n",
        "            else:\n",
        "                outputs = outputs[:, -1, :]\n",
        "\n",
        "        output = F.sigmoid(self.logit(outputs_kg if self.use_knowledge_graph else outputs))\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "        # calculate contributions\n",
        "\n"
      ],
      "metadata": {
        "id": "Big1DECYHLeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Validation"
      ],
      "metadata": {
        "id": "b9CjfB6IrSLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we start training:\n",
        "* This portion handles the training and testing process of the model.\n",
        "* The train function is responsible for training the model, while the test * function evaluates the model on the validation set.\n",
        "* The main function orchestrates the overall process, including data loading, model initialization, training, and testing."
      ],
      "metadata": {
        "id": "6fyiM-nIez4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def evaluate(net, loader):\n",
        "\n",
        "    net.eval()\n",
        "    y_score = torch.Tensor()\n",
        "    y_true = torch.LongTensor()\n",
        "    for batch in loader:\n",
        "        conditions = torch.tensor(batch['conditions']).to(device)\n",
        "        masks = torch.tensor(batch['masks'], dtype=torch.bool).to(device)\n",
        "        label = torch.tensor(batch['label'], dtype=torch.float32)\n",
        "\n",
        "        y = net(conditions, masks).view(-1)\n",
        "\n",
        "        y_score = torch.cat((y_score, y.detach().to('cpu')), dim=0)\n",
        "        y_true = torch.cat((y_true, label), dim=0)\n",
        "\n",
        "    roc_auc = roc_auc_score(y_true, y_score)\n",
        "    return roc_auc\n"
      ],
      "metadata": {
        "id": "sYwnXgEok7fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "\n",
        "# settings\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "# initialize dg-rnn model\n",
        "net = FCModel().to(device)\n",
        "\n",
        "# initialize loss\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# initialize optimizer\n",
        "optimizer = torch.optim.Adam(net.parameters(), learning_rate)\n",
        "\n",
        "\n",
        "net.train()\n",
        "for epoch in range(epochs):\n",
        "        train_loss = 0\n",
        "        for batch in train_loader:\n",
        "            conditions = torch.tensor(batch['conditions']).to(device)\n",
        "            masks = torch.tensor(batch['masks'], dtype=torch.bool).to(device)\n",
        "            label = torch.tensor(batch['label'], dtype=torch.float32).view((-1,1)).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            y = net(conditions, masks)\n",
        "            loss = criterion(y, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        roc_auc = evaluate(net, val_loader)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1} \\t Train Loss: {'{:.3f}'.format(train_loss)} \\t Validation AUROC: {'{:.3f}'.format(roc_auc)}\")\n"
      ],
      "metadata": {
        "id": "rtPpSPqgD2hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "YSYWhgFAsmDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We evaluated the model as below:\n",
        "\n",
        "* This portion below is used for making predictions with a trained DG-RNN model. Given new EHR data, it uses the model to predict clinical outcomes/risks, such as the likelihood of heart failure.\n",
        "\n",
        "* This portion contains the code for performing inference using the trained model on the test set.\n",
        "\n"
      ],
      "metadata": {
        "id": "gyAJQkZorMK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_roc_auc = evaluate(net, test_loader)\n",
        "print(f\"Test AUROC: {test_roc_auc}\")"
      ],
      "metadata": {
        "id": "AX7gT3qQotlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(net.parameters)"
      ],
      "metadata": {
        "id": "iyRyl2UB1Urj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results"
      ],
      "metadata": {
        "id": "Nm0FqF90F36q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing conditions:**\n",
        "\n",
        "1. Model without pretrained embeddings, without knowledge graph, without graph attention, and without global pooling\n",
        "\n",
        "2. Model without graph attention and without global pooling\n",
        "\n",
        "3. Model without global pooling\n",
        "\n",
        "4. Model with pretrained embeddings, with knowledge graph, with graph attention, and with global pooling"
      ],
      "metadata": {
        "id": "7JO1NgdsFpQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "experiments = [\n",
        "    {'name': 'Baseline', 'pretrained_embeddings': False, 'knowledge_graph': False, 'graph_attention': False, 'global_pooling': False},\n",
        "    {'name': 'Knowledge Graph', 'pretrained_embeddings': True, 'knowledge_graph': True, 'graph_attention': False, 'global_pooling': False},\n",
        "    {'name': 'Graph Attention', 'pretrained_embeddings': True, 'knowledge_graph': True, 'graph_attention': True, 'global_pooling': False},\n",
        "    {'name': 'Global Pooling', 'pretrained_embeddings': True, 'knowledge_graph': True, 'graph_attention': True, 'global_pooling': True}\n",
        "]\n",
        "\n",
        "# Initialize a list to store the results\n",
        "results = []\n",
        "\n",
        "# Perform experiments\n",
        "for exp in experiments:\n",
        "    # Set the model parameters based on the experiment settings\n",
        "    net.load_pretrained_embeddings = exp['pretrained_embeddings']\n",
        "    net.use_knowledge_graph = exp['knowledge_graph']\n",
        "    net.use_graph_attention = exp['graph_attention']\n",
        "    net.use_global_pooling = exp['global_pooling']\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0\n",
        "        for batch in train_loader:\n",
        "            conditions = torch.tensor(batch['conditions']).to(device)\n",
        "            masks = torch.tensor(batch['masks'], dtype=torch.bool).to(device)\n",
        "            label = torch.tensor(batch['label'], dtype=torch.float32).view((-1,1)).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            y = net(conditions, masks)\n",
        "            loss = criterion(y, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        roc_auc = evaluate(net, val_loader)\n",
        "        print(f\"Epoch: {epoch+1} \\t Train Loss: {'{:.3f}'.format(train_loss)} \\t Validation AUROC: {'{:.3f}'.format(roc_auc)}\")\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    test_roc_auc = evaluate(net, test_loader)\n",
        "    print(f\"Test AUROC: {test_roc_auc}\")\n",
        "\n",
        "    # Store the results\n",
        "    results.append({'Experiment': exp['name'], 'Test AUROC': test_roc_auc})\n",
        "\n",
        "# Create a DataFrame with the results\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(results_df['Experiment'], results_df['Test AUROC'])\n",
        "plt.xlabel('Experiment')\n",
        "plt.ylabel('Test AUROC')\n",
        "plt.title('Performance Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_pDHwPkRPLmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analyses"
      ],
      "metadata": {
        "id": "oeXITfb_O31a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analyses is done based on our first several runs of this notebook. The result is attached here:\n",
        "\n",
        "```\n",
        "Baseline AUROC: 0.549899\n",
        "\n",
        "Knowledge Graph AUROC: 0.566563\n",
        "\n",
        "Graph Attention AUROC:0.550266\n",
        "\n",
        "Global Pooling AUROC: 0.775682\n",
        "```\n",
        "\n",
        "**Hypothesis 1:** Incorporating domain knowledge from a medical knowledge graph (KnowLife or UMLS) into a deep learning model (DG-RNN) will significantly improve the performance of clinical risk prediction compared to models that do not utilize domain knowledge.\n",
        "\n",
        "* The experiment \"Baseline\" represents the model without using pretrained embeddings, knowledge graph, graph attention, or global pooling. It achieved a test AUROC of 0.549899.\n",
        "* The experiment \"Knowledge Graph\" incorporates the knowledge graph along with pretrained embeddings, resulting in a test AUROC of 0.566563.\n",
        "* The improvement in performance from \"Baseline\" to \"Knowledge Graph\" supports the hypothesis that incorporating domain knowledge from a medical knowledge graph enhances the model's ability to predict clinical risk.\n",
        "\n",
        "**Hypothesis 2:** The dynamic attention mechanism in DG-RNN, which integrates relevant information from the medical knowledge graph at each step, will contribute to better risk prediction performance compared to models with static or no attention mechanisms.\n",
        "\n",
        "* The experiment \"Graph Attention\" adds the graph attention mechanism to the model, which dynamically attends to relevant information from the knowledge graph at each step.\n",
        "* Comparing the test AUROC of \"Graph Attention\" (0.550266) with \"Knowledge Graph\" (0.566563) shows a slight decrease in performance.\n",
        "* The results do not provide strong support for the hypothesis that the dynamic attention mechanism contributes to better risk prediction performance compared to models without attention mechanisms.\n",
        "\n",
        "**Hypothesis 3:** The global max pooling operation in DG-RNN will not only improve the model's performance but also enable the interpretation of individual medical event contributions to the predicted risk.\n",
        "\n",
        "* The experiment \"Global Pooling\" includes the global max pooling operation in the model, along with pretrained embeddings, knowledge graph, and graph attention.\n",
        "* The test AUROC of \"Global Pooling\" (0.775682) is higher than all the previous experiments, indicating an improvement in performance.\n",
        "* The global max pooling operation allows for the interpretation of individual medical event contributions to the predicted risk by identifying the most salient features across the sequence.\n",
        "\n",
        "**Hypothesis 4:** DG-RNN will demonstrate robustness to limited training data, outperforming baseline models even when the amount of available training data is reduced.\n",
        "\n",
        "* To test this hypothesis, additional experiments would be needed with varying amounts of training data to compare the performance of DG-RNN against baseline models.\n",
        "* The provided results do not include experiments with reduced training data, so we cannot draw conclusions about the robustness of DG-RNN in such scenarios based on the available information."
      ],
      "metadata": {
        "id": "k9QNg0-oF5F7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plans"
      ],
      "metadata": {
        "id": "5LQsp2imO9MC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case-control Matching:**\n",
        "\n",
        "In the final report, we will use the case-control matching function to preprocess the full dataset instead of the smaller demo dataset.\n",
        "\n",
        "**Time Encoding:**\n",
        "1. The current implementation of the DG-RNN model does not include time encoding to capture the irregular time intervals between patient visits.\n",
        "2. For the future plan, we will incorporate time encoding into the model to capture the temporal aspects of EHR data. This can be done by adding a time encoding component similar to the positional encodings used in the Transformer architecture, as mentioned in the paper.\n",
        "\n",
        "**Ablation Studies:**\n",
        "\n",
        "To gain a deeper understanding of the impact of each component in the DG-RNN model, we propose conducting a series of ablation studies. These studies will systematically remove or modify specific components of the model and evaluate the resulting changes in performance and interpretability. The planned ablation experiments include:\n",
        "\n",
        "1.   Removing Time Encoding: This experiment aims to assess the importance of incorporating time interval information between patient visits. By treating all events as occurring simultaneously, we can evaluate the model's performance without considering the temporal aspects of EHR data. We hypothesize that removing time encoding may lead to a decrease in performance, especially for conditions where the timing of events plays a crucial role in accurate risk assessment.\n",
        "2.   Removing Medical Knowledge Graph: In this experiment, we will disable the medical knowledge graph and the associated attention mechanism. By doing so, we can evaluate the contribution of domain-specific insights and relationships captured by the knowledge graph to the model's performance. We expect a decline in clinical risk prediction accuracy when the knowledge graph is removed, as the model will lack the additional context and understanding of medical relationships.\n",
        "3.   Removing Medical Entity and Relation Embeddings: This ablation study involves training the DG-RNN model without the use of pre-trained embeddings for medical entities and relations. Instead, the model will learn solely from the raw EHR data. We anticipate a decrease in performance due to the absence of the rich domain knowledge and semantic relationships encoded in the pre-trained embeddings. This experiment will highlight the importance of leveraging pre-existing knowledge in improving the model's ability to capture complex medical interrelations.\n",
        "4.   Removing Global Max Pooling Layer: The global max pooling layer plays a crucial role in identifying the most salient features across the sequence of medical events and enabling interpretability. By removing this layer, we aim to investigate its impact on the model's performance and its ability to provide insights into the contributions of individual medical events to the predicted outcomes. We hypothesize that the absence of the global max pooling layer may hinder interpretability and affect the efficiency of gradient propagation during training.\n",
        "\n",
        "\n",
        "**Contribution Analysis:**\n",
        "\n",
        "For the future plan, we will perform a thorough contribution analysis to gain insights into the importance of individual medical events in the predicted outcomes.\n",
        "\n",
        "Steps to do the contribution analysis may involve the following steps:\n",
        "  1. Obtain the contribution risks of each dimension in the final patient representation vector `(h_c)` by analyzing the outputs of the fully connected layer.\n",
        "  2. Trace back the contribution risks to the corresponding output vectors of the LSTM layer.\n",
        "  3. Calculate the contribution rate of each input medical event by summing the contribution risks of its corresponding output vectors.\n",
        "  4. Visualize the contribution rates of individual medical events using bar plots or heatmaps to provide interpretable insights into the model's decision-making process.\n",
        "\n"
      ],
      "metadata": {
        "id": "YRNENvAxOgFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Hv-TWtrBnJIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   C. Yin, R. Zhao, B. Qian, X. Lv and P. Zhang, \"Domain Knowledge Guided Deep Learning with Electronic Health Records,\" 2019 IEEE International Conference on Data Mining (ICDM), Beijing, China, 2019, pp. 738-747, doi: 10.1109/ICDM.2019.00084"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}